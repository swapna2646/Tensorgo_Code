import cv2
import tensorflow as tf
from tensorflow.python.framework import graph_util

# Load the pre-trained GPU-based OCR model
gpu_model = cv2.dnn.readNetFromTensorflow('gpu_ocr_model.pb')

# Convert the model to run on CPU
cpu_model = tf.graph_util.convert_variables_to_constants(gpu_model, ['output'])

# Save the converted CPU-based OCR model
tf.train.write_graph(cpu_model, 'cpu_ocr_model', 'cpu_ocr_model.pb', False)

# Load the video file
video_file = 'input_video.mp4'
cap = cv2.VideoCapture(video_file)

# Process the video file using both GPU and CPU models
while True:
    ret, frame = cap.read()
    if not ret:
        break

    # GPU-based OCR model
    gpu_output = gpu_model.forward(frame)
    gpu_text = cv2.OCR.decode(gpu_output)

    # CPU-based OCR model
    cpu_output = cpu_model.forward(frame)
    cpu_text = cv2.OCR.decode(cpu_output)

    # Display the output
    cv2.imshow('GPU Output', gpu_text)
    cv2.imshow('CPU Output', cpu_text)

    # Press 'q' to quit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release resources
cap.release()
cv2.destroyAllWindows()

# Comparative analysis
gpu_fps = cap.get(cv2.CAP_PROP_FPS)
cpu_fps = cap.get(cv2.CAP_PROP_FPS)

print('GPU FPS:', gpu_fps)
print('CPU FPS:', cpu_fps)

print('GPU Accuracy:', gpu_model.get_accuracy())
print('CPU Accuracy:', cpu_model.get_accuracy())

print('GPU Resource Utilization:', gpu_model.get_resource_utilization())
print('CPU Resource Utilization:', cpu_model.get_resource_utilization())